services:
  text2vec:
    image: ${TEXT2VEC_VACC_IMAGE}
    container_name: text2vec
    restart: unless-stopped  # 更智能的重启策略
    shm_size: 256g
    privileged: true
    ports:
      - "9996:9996"
    cap_add:
      - SYS_NICE
    environment:
      # 稳定性关键配置
      - XINFERENCE_HEARTBEAT_INTERVAL=60      # 心跳检测间隔(秒)
      - XINFERENCE_WORKER_TIMEOUT=86400       # Worker无响应超时(24小时)
      - VLLM_ENGINE_ITERATION_TIMEOUT_S=86400 # vLLM引擎超时(24小时)
      - XINFERENCE_SSE_PING_ATTEMPTS_SECONDS=864000
      - VLLM_NO_USAGE_TIMEOUT=0               # 禁用无请求超时
      - XINFERENCE_SCHEDULER_POLICY=least_loaded  # 最小负载调度策略
      - XINFERENCE_REPLICA_BALANCE=true          # 启用副本均衡
      - XINFERENCE_WORKER_PERSISTENT=true
      - XINFERENCE_LOAD_METRIC=gpu_util  # 按GPU利用率而非请求数判断负载
      - XINFERENCE_LOAD_THRESHOLD=90     # 超过90%利用率视为高负载
      - model_path=mod
      - embed_model_name=${embed_model_name}
      - rerank_model_name=${rerank_model_name}
      - embed_model_len=${model_len}
      - embed_GPUs=${embed_GPUs}
      - rerank_GPUs=${rerank_GPUs}
      - embed_instance_nums=${embed_instance_nums}
      - rerank_instance_nums=${rerank_instance_nums}

    volumes:
      - ${HOST_EMBED_DATA_DIR}:/embed_weights/
      - ${HOST_RERANK_DATA_DIR}:/rerank_weights/
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://0.0.0.0:9996/status || exit 1"]
      interval: 30s
      timeout: 20s
      retries: 3
    entrypoint:
      - /bin/bash
      - -c
      - |
        # 防止SIGTERM导致退出
        trap : TERM INT
        unset VACC_VISIBLE_DEVICES
        source /opt/vastai/vaststream/set_env.sh

        cd /test/vsx 

        echo 'VASTAI_EMBEDDING=$$embed_GPUs VASTAI_RERANK=$$rerank_GPUs xinference-local --host 0.0.0.0 --port 9996' > xinf_local.sh

        bash xinf_local.sh &

        # 等待服务就绪
        while ! curl -s http://127.0.0.1:9996 >/dev/null; do
          sleep 2
        done
        echo '{
          "model_name": "'$$embed_model_name'",
          "model_uid": "'$$embed_model_name'",
          "dimensions": 1024,
          "max_tokens": '$$embed_model_len',
          "language": ["en"],
          "model_specs": [
            {
                "model_uri": "/embed_weights/'$${model_path}'",
                "model_format": "pytorch",
                "quantization": "none"
            }
          ]
        }' > register_emb.json

        python test_client.py --url http://127.0.0.1:9996 --embedding --model-config register_emb.json --n_gpu 1 --instance-nums $$embed_instance_nums 

        echo '{
          "model_name": "'$$rerank_model_name'",
          "model_uid": "'$$rerank_model_name'",
          "language": ["en"],
          "model_uri": "/rerank_weights/'$${model_path}'"
        }' > register_rerank.json

        python test_client.py --url http://127.0.0.1:9996 --rerank --model-config register_rerank.json --n_gpu 1 --instance-nums $$rerank_instance_nums 
        
        tail -f /dev/null 
